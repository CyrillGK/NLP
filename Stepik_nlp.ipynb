{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stepik_nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1HonDzwgGbzj0ra3gdUa06ZZKKDpuXbzi",
      "authorship_tag": "ABX9TyMptcHvG6sr4Zw6vfO4sCs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CyrillGK/NLP/blob/main/Stepik_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dPlLfRwHTdu"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import copy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHlSTU7cw7ax",
        "outputId": "4a5d6355-8bac-4af9-fd27-0bb470f8d795"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BZsCjxmor1q"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iuvv8O2_PBq3",
        "outputId": "e3dc0103-99bd-489b-fb4c-8351adfdab2a"
      },
      "source": [
        "cd drive/MyDrive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGVIkween3D_",
        "outputId": "9f900c7e-dcd5-4eaf-d927-9f704e193b4f"
      },
      "source": [
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "import tools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ipynb\n",
            "  Downloading https://files.pythonhosted.org/packages/63/35/495e0021bfdcc924c7cdec4e9fbb87c88dd03b9b9b22419444dc370c8a45/import-ipynb-0.1.3.tar.gz\n",
            "Building wheels for collected packages: import-ipynb\n",
            "  Building wheel for import-ipynb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for import-ipynb: filename=import_ipynb-0.1.3-cp37-none-any.whl size=2976 sha256=dbae9d143b1279098fa25f8d19c2001f555822ffe5f167db64e071e510388780\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/e9/a3a6e496115dffdb4e3085d0ae39ffe8a814eacc44bbf494b5\n",
            "Successfully built import-ipynb\n",
            "Installing collected packages: import-ipynb\n",
            "Successfully installed import-ipynb-0.1.3\n",
            "importing Jupyter notebook from tools.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR54yZRyKa1m"
      },
      "source": [
        "https://github.com/Samsung-IT-Academy/stepik-dl-nlp/blob/master/task1_20newsgroups.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90-vczT1HiQB"
      },
      "source": [
        "## Классификация новостных сайтов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRcsDO5zVwx8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c808034-b4b4-42bd-918a-be5fd19fa218"
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "len(train_source['data']), len(test_source['data'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 7532)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPnSrbNf4SyF"
      },
      "source": [
        "### Линейная модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkzbhqUf4atr"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC, SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwSCC5q-3qDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "920f47cd-48e2-4a06-a069-6126b0800241"
      },
      "source": [
        "res = pd.DataFrame(columns=['CE_train', 'F1_train', 'CE_test', 'F1_test'])\n",
        "pipe = Pipeline((\n",
        "    ('vect', TfidfVectorizer()),\n",
        "    ('cls', LinearSVC())\n",
        "))\n",
        "pipe.fit(train_source['data'], train_source['target']);\n",
        "train_pred = pipe.predict(train_source['data'])\n",
        "test_pred = pipe.predict(test_source['data'])\n",
        "\n",
        "#train_ce = float(F.cross_entropy(torch.from_numpy(train_pred), torch.from_numpy(train_source['target'])))\n",
        "train_f1 = f1_score(train_source['target'], train_pred, average='weighted')\n",
        "#test_ce = float(F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target'])))\n",
        "test_f1 = f1_score(test_source['target'], test_pred, average='weighted')\n",
        "train_f1, test_f1\n",
        "#res = res.append({'CE_train':train_ce, 'F1_train':train_f1, 'CE_test':test_ce, 'F1_test':test_f1}, ignore_index=True)\n",
        "#res"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9990279582521118, 0.851876062902485)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdIG-Dq35hVo"
      },
      "source": [
        "### Линейный слой на TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBelPTecophi"
      },
      "source": [
        "import tools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVshPS-P5fTP"
      },
      "source": [
        "tfidf = TfidfVectorizer(max_features=1500)\n",
        "X_train = tfidf.fit_transform(train_source['data'], train_source['target'])\n",
        "X_test = tfidf.transform(test_source['data'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYV2ZF0axETB"
      },
      "source": [
        "class SparseFeaturesDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.features.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cur_features = torch.from_numpy(self.features[idx].toarray()[0]).float()\n",
        "        cur_label = torch.from_numpy(np.asarray(self.targets[idx])).long()\n",
        "        return cur_features, cur_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7tm5YwoxApY"
      },
      "source": [
        "train_dataset = SparseFeaturesDataset(X_train, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(X_test, test_source['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOWBXq8VxqZ0"
      },
      "source": [
        "class NLPNet(torch.nn.Module):\n",
        "    def __init__(self, n_hidden_neurons):\n",
        "        super(NLPNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(1500, n_hidden_neurons)\n",
        "        self.act1 = torch.nn.Sigmoid()\n",
        "        self.fc2 = torch.nn.Linear(n_hidden_neurons, 20)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBGBze77yXUw"
      },
      "source": [
        "model = NLPNet(750)\n",
        "#model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roCa3s3hyT5h"
      },
      "source": [
        "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
        "                    device='cpu', early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000, max_batches_per_epoch_val=1000,\n",
        "                    shuffle_train=True, collate_fn=None):\n",
        "    \"\"\"\n",
        "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
        "    :param model: torch.nn.Module - обучаемая модель\n",
        "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
        "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
        "    :param criterion: функция потерь для настройки модели\n",
        "    :param lr: скорость обучения\n",
        "    :param epoch_n: максимальное количество эпох\n",
        "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
        "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
        "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
        "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
        "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
        "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
        "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
        "    :return: кортеж из двух элементов:\n",
        "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
        "        - лучшая модель\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        epoch_start = datetime.datetime.now()\n",
        "        print('Эпоха {}'.format(epoch_i))\n",
        "\n",
        "        model.train()\n",
        "        mean_train_loss = 0\n",
        "        train_batches_n = 0\n",
        "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "            if batch_i > max_batches_per_epoch_train:\n",
        "                break\n",
        "\n",
        "            batch_x = tools.copy_data_to_device(batch_x, device)\n",
        "            batch_y = tools.copy_data_to_device(batch_y, device)\n",
        "\n",
        "            pred = model(batch_x)\n",
        "            loss = criterion(pred, batch_y)\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            mean_train_loss += float(loss)\n",
        "            train_batches_n += 1\n",
        "\n",
        "        mean_train_loss /= train_batches_n\n",
        "        print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n, (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "        print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        mean_val_loss = 0\n",
        "        val_batches_n = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_val:\n",
        "                    break\n",
        "\n",
        "                batch_x = tools.copy_data_to_device(batch_x, device)\n",
        "                batch_y = tools.copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "\n",
        "                mean_val_loss += float(loss)\n",
        "                val_batches_n += 1\n",
        "\n",
        "        mean_val_loss /= val_batches_n\n",
        "        print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
        "\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best_epoch_i = epoch_i\n",
        "            best_val_loss = mean_val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            print('Новая лучшая модель!')\n",
        "        elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "            print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
        "                early_stopping_patience))\n",
        "            break\n",
        "\n",
        "        lr_scheduler.step(mean_val_loss)\n",
        "        print()\n",
        "\n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX4mk_g6LHbo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "614f9ba8-8e75-4855-b85d-361d8b08ab47"
      },
      "source": [
        "_, best_model = train_eval_loop(model=model,\n",
        "                                train_dataset=train_dataset,\n",
        "                                val_dataset=test_dataset,\n",
        "                                criterion=F.cross_entropy,\n",
        "                                lr=1e-1,\n",
        "                                epoch_n=30,\n",
        "                                batch_size=32,\n",
        "                                l2_reg_alpha=1e-5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха 0\n",
            "Эпоха: 354 итераций, 4.93 сек\n",
            "Среднее значение функции потерь на обучении 0.6297133692492873\n",
            "Среднее значение функции потерь на валидации 1.4333481806314599\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 1\n",
            "Эпоха: 354 итераций, 4.74 сек\n",
            "Среднее значение функции потерь на обучении 0.4961384821704215\n",
            "Среднее значение функции потерь на валидации 1.554447521092528\n",
            "\n",
            "Эпоха 2\n",
            "Эпоха: 354 итераций, 4.80 сек\n",
            "Среднее значение функции потерь на обучении 0.48384858800445574\n",
            "Среднее значение функции потерь на валидации 1.558987007050191\n",
            "\n",
            "Эпоха 3\n",
            "Эпоха: 354 итераций, 4.98 сек\n",
            "Среднее значение функции потерь на обучении 0.46802780195270893\n",
            "Среднее значение функции потерь на валидации 1.5466190515938452\n",
            "\n",
            "Эпоха 4\n",
            "Эпоха: 354 итераций, 4.93 сек\n",
            "Среднее значение функции потерь на обучении 0.43811458911178475\n",
            "Среднее значение функции потерь на валидации 1.6619163648051731\n",
            "\n",
            "Эпоха 5\n",
            "Эпоха: 354 итераций, 4.73 сек\n",
            "Среднее значение функции потерь на обучении 0.4497166555480095\n",
            "Среднее значение функции потерь на валидации 1.5773290913488904\n",
            "\n",
            "Эпоха 6\n",
            "Эпоха: 354 итераций, 4.75 сек\n",
            "Среднее значение функции потерь на обучении 0.4320036611482922\n",
            "Среднее значение функции потерь на валидации 1.576248656390077\n",
            "Epoch     7: reducing learning rate of group 0 to 5.0000e-02.\n",
            "\n",
            "Эпоха 7\n",
            "Эпоха: 354 итераций, 4.72 сек\n",
            "Среднее значение функции потерь на обучении 0.2536656672482268\n",
            "Среднее значение функции потерь на валидации 1.5270784307839507\n",
            "\n",
            "Эпоха 8\n",
            "Эпоха: 354 итераций, 4.65 сек\n",
            "Среднее значение функции потерь на обучении 0.15580819678192925\n",
            "Среднее значение функции потерь на валидации 1.5207527481903464\n",
            "\n",
            "Эпоха 9\n",
            "Эпоха: 354 итераций, 4.71 сек\n",
            "Среднее значение функции потерь на обучении 0.17895403375459762\n",
            "Среднее значение функции потерь на валидации 1.6574458724866479\n",
            "\n",
            "Эпоха 10\n",
            "Эпоха: 354 итераций, 4.68 сек\n",
            "Среднее значение функции потерь на обучении 0.23951673839178125\n",
            "Среднее значение функции потерь на валидации 1.6176310113425982\n",
            "\n",
            "Эпоха 11\n",
            "Эпоха: 354 итераций, 4.72 сек\n",
            "Среднее значение функции потерь на обучении 0.2278618865809535\n",
            "Среднее значение функции потерь на валидации 1.7339769763461614\n",
            "Модель не улучшилась за последние 10 эпох, прекращаем обучение\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlY-P3NwL8Fm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "390ee8b3-aab9-4c23-819d-356e58afb35d"
      },
      "source": [
        "train_pred = tools.predict_with_model(best_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred), torch.from_numpy(train_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "test_pred = tools.predict_with_model(best_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(test_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.3254581093788147\n",
            "Доля верных ответов 0.8986211773024572\n",
            "\n",
            "Среднее значение функции потерь на валидации 1.4330660104751587\n",
            "Доля верных ответов 0.6311736590546999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9v_e8VJ0FFv"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJkxl1yf0P5G"
      },
      "source": [
        "from torchtext.vocab import Vocab\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvYs8akT0LZX"
      },
      "source": [
        "tokenize = TfidfVectorizer().build_analyzer()\n",
        "vocab = Vocab(\n",
        "    Counter(\n",
        "        tokenize(' '.join(train_source['data']))\n",
        "    )\n",
        ", min_freq=1, max_size=20000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OmLqYTE0qLv"
      },
      "source": [
        "text_pipeline = lambda x: [vocab.freqs[token] for token in tokenize(x)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbt6HplL0yk9"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list = []\n",
        "    for (_text, _label) in batch:\n",
        "         label_list.append(_label)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    vectorized_seqs = [text_pipeline(seq[0]) for seq in batch]\n",
        "    seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "    seq_tensor = torch.autograd.Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "        seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "    seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "    seq_tensor = seq_tensor[perm_idx]\n",
        "    return (seq_tensor.to(device), seq_lengths), label_list.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGzvP3EqM0k7"
      },
      "source": [
        "#### Линейный слой с Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxVByEgntuVk"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return (text_list.to(device), offsets.to(device)), label_list.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXJSu8QL01rM"
      },
      "source": [
        "class LinearEmbNet(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim):\n",
        "        super(LinearEmbNet, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, emb_dim)\n",
        "        self.fc2 = torch.nn.Linear(emb_dim, 20)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EcFP5rH1I-W"
      },
      "source": [
        "model = LinearEmbNet(len(vocab), 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8JVPQJu1L-4"
      },
      "source": [
        "_, best_model = train_eval_loop(model=model,\n",
        "                                train_dataset=train_dataset,\n",
        "                                val_dataset=test_dataset,\n",
        "                                criterion=F.cross_entropy,\n",
        "                                lr=1e-1,\n",
        "                                epoch_n=30,\n",
        "                                batch_size=32,\n",
        "                                l2_reg_alpha=1e-5,\n",
        "                                collate_fn=collate_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSmgfwa35svj"
      },
      "source": [
        "### Сверточный слой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZkWppLX5yWt"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4qEX7qkK08r"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0], embedding_dim))\n",
        "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1], embedding_dim))\n",
        "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2], embedding_dim))\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #x = [sent len, batch size]\n",
        "        x = x.permute(1, 0)\n",
        "                \n",
        "        #x = [batch size, sent len]\n",
        "        embedded = self.embedding(x)\n",
        "                \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "        \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
        "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
        "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
        "            \n",
        "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
        "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
        "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
        "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
        "        \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        cat = torch.cat((pooled_0, pooled_1, pooled_2), dim=1)\n",
        "\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuFaKzOxM7zD"
      },
      "source": [
        "### Рекуррентный слой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrOC1f1INCg-"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB5ahkt0tYYH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, n_layers):\n",
        "        super(NewModel, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.gru = nn.GRU(input_size=embed_dim, hidden_size=hidden_size, num_layers=n_layers, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(2*hidden_size, 98)    \n",
        "        \n",
        "        def init_hidden(self, batch_size):\n",
        "          weight = next(self.parameters()).data\n",
        "          hidden = weight.new(2*self.n_layers, batch_size, self.hidden_size).zero_().to(device)\n",
        "        return hidden    \n",
        "        \n",
        "        def forward(self, data, h):\n",
        "          embedded = self.embedding(data[0])\n",
        "          x, h = self.gru(embedded, h)\n",
        "          #print(x[:,-1].shape)\n",
        "        return self.fc(x[:,-1]), h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsFiIar8ta6c"
      },
      "source": [
        "def train_eval_l(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32, device='cpu', early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000, max_batches_per_epoch_val=1000, shuffle_train=True, collate_fn=None):    \n",
        "    device = torch.device(device)\n",
        "    model.to(device)    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=True)    \n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_train, collate_fn=collate_fn)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)    \n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)    \n",
        "    for epoch_i in range(epoch_n):\n",
        "        epoch_start = datetime.datetime.now()\n",
        "        print('Эпоха {}'.format(epoch_i))\n",
        "        h = model.init_hidden(batch_size)        \n",
        "        model.train()\n",
        "        mean_train_loss = train_batches_n = 0\n",
        "        for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "            h = h.data\n",
        "            if batch_i > max_batches_per_epoch_train:\n",
        "                break\n",
        "            batch_x = tools.copy_data_to_device(batch_x, device)\n",
        "            batch_y = tools.copy_data_to_device(batch_y, device)\n",
        "            model.zero_grad()\n",
        "            pred, h = model(batch_x, h)\n",
        "            #pred = model(batch_x)\n",
        "            loss = criterion(pred, batch_y)            \n",
        "            loss.backward()            \n",
        "            optimizer.step()            \n",
        "            mean_train_loss += float(loss)\n",
        "            train_batches_n += 1        \n",
        "            mean_train_loss /= train_batches_n\n",
        "        print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n, (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "        print('Среднее значение функции потерь на обучении', mean_train_loss)        \n",
        "        model.eval()\n",
        "        mean_val_loss = val_batches_n = 0        \n",
        "        with torch.no_grad():\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_val:\n",
        "                    break\n",
        "                h = model.init_hidden(len(batch_y))\n",
        "                batch_x = tools.copy_data_to_device(batch_x, device)\n",
        "                batch_y = tools.copy_data_to_device(batch_y, device)                \n",
        "                pred, h = model(batch_x, h)\n",
        "             #   pred = model(batch_x)                \n",
        "                loss = criterion(pred, batch_y)                \n",
        "                mean_val_loss += float(loss)\n",
        "                val_batches_n += 1        \n",
        "                mean_val_loss /= val_batches_n\n",
        "        print('Среднее значение функции потерь на валидации', mean_val_loss)        \n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best_epoch_i = epoch_i\n",
        "            best_val_loss = mean_val_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            print('Новая лучшая модель!')\n",
        "        elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "            print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(early_stopping_patience))\n",
        "            break        \n",
        "        lr_scheduler.step(mean_val_loss)\n",
        "        print()    \n",
        "    return best_val_loss, best_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-jPBtZbte6n"
      },
      "source": [
        "def predict_with_model(model, dataset, device=None, batch_size=32, return_labels=False, collate_fn=None):\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results_by_batch = []    \n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()    \n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in dataloader:            \n",
        "            batch_x = tools.copy_data_to_device(batch_x, device)            \n",
        "            if return_labels:\n",
        "                labels.append(batch_y.numpy())\n",
        "            h = model.init_hidden(len(batch_y))\n",
        "            batch_pred, h = model(batch_x, h)\n",
        "            results_by_batch.append(batch_pred.detach().cpu().numpy())    \n",
        "    if return_labels:\n",
        "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
        "    else:\n",
        "        return np.concatenate(results_by_batch, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHKN1aDTthNU"
      },
      "source": [
        "train_pred = predict_with_model(\n",
        "    best_model,\n",
        "    list(zip(train.item_name, [classes[i] for i in train['category_id']])),\n",
        "    collate_fn=collate_batch,\n",
        "    device=device\n",
        ")\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred), torch.from_numpy(np.array(y_train)).long())\n",
        "print('BCE_train', float(train_loss))\n",
        "print('ACC_train', f1_score(y_train, train_pred.argmax(-1), average='weighted'))\n",
        "print()\n",
        "test_pred = predict_with_model(\n",
        "    best_model,\n",
        "    list(zip(test.item_name, [classes[i] for i in test['category_id']])),\n",
        "    collate_fn=collate_batch,\n",
        "    device=device\n",
        ")\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred), torch.from_numpy(np.array(y_test)).long())\n",
        "print('BCE_test', float(test_loss))\n",
        "print('ACC_test', f1_score(y_test, test_pred.argmax(-1), average='weighted'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ACur4dbCNc"
      },
      "source": [
        "## Word2Vec и Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B7Kak_NbRml"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vRwenlDbtqi"
      },
      "source": [
        "full_dataset = list(pd.read_csv('drive/MyDrive/Colab Notebooks/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
        "random.shuffle(full_dataset)\n",
        "\n",
        "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
        "train_source, test_source = full_dataset[:TRAIN_VAL_SPLIT], full_dataset[TRAIN_VAL_SPLIT:]\n",
        "len(train_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uf50_Zcb6wu"
      },
      "source": [
        "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZXQPwyltypK"
      },
      "source": [
        "# word2vec.wv.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dO7mZB-u2MH"
      },
      "source": [
        "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', \n",
        "              'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRF89KFyt5Rn"
      },
      "source": [
        "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
        "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZmJ0Htdu7NE"
      },
      "source": [
        "fasttext = gensim.models.FastText(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRSLdCMuvf90"
      },
      "source": [
        "# fasttext.wv.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6dg5fxZvlmS"
      },
      "source": [
        "ft_words = [w for w in test_words if w in fasttext.wv.vocab]\n",
        "ft_vectors = np.stack([fasttext.wv[w] for w in ft_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9clRRnU4v0hG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}